"""The input / output functions related to file system."""
from pathlib import Path
from typing import Dict, Any

import fabio
import pyFAI
import yaml
from diffpy.pdfgetx import PDFConfig, PDFGetter
from numpy import ndarray


def load_ai_from_poni_file(poni_file: str) -> pyFAI.AzimuthalIntegrator:
    """Initiate the AzimuthalIntegrator using poni file."""
    ai = pyFAI.load(poni_file)
    return ai


def load_ai_from_calib_result(calib_result: dict) -> pyFAI.AzimuthalIntegrator:
    """Initiate the AzimuthalIntegrator using calibration information."""
    ai = pyFAI.AzimuthalIntegrator()
    ai.set_config(calib_result)
    return ai


def load_img(img_file: str) -> ndarray:
    """Load the img data from the img_file."""
    img = fabio.open(img_file).data
    return img


def load_pdfconfig(cfg_file: str) -> PDFConfig:
    """Load the PDFConfig from the processed data file or configuration file."""
    pdfconfig = PDFConfig()
    pdfconfig.readConfig(cfg_file)
    return pdfconfig


def write_pdfgetter(saving_dir: str, filename: str, pdfgetter: PDFGetter) -> dict:
    """Write out data in pdfgetter into files"""
    data_dirs = {}
    for out_type in pdfgetter.config.outputtypes:
        data_dir = Path(saving_dir).joinpath(out_type)
        if not data_dir.exists():
            data_dir.mkdir()
        data_dirs.update({out_type: data_dir})
    dct = {}
    for out_type in pdfgetter.config.outputtypes:  # out_type in ('iq', 'sq', 'fq', 'gr')
        data_dir = data_dirs.get(out_type)
        out_file = data_dir.joinpath(Path(filename).with_suffix(".{}".format(out_type)).name)
        pdfgetter.writeOutput(str(out_file), out_type)
        dct.update({out_type: str(out_file)})
    return dct


def write_img(filepath: str, img: ndarray, template: str) -> None:
    """Write out the image data as the same type of the template file."""
    temp_img = fabio.open(template)
    temp_img.data = img
    temp_img.save(filepath)
    return


def load_array(data_file: str, minrows=10, **kwargs) -> ndarray:
    """Load data columns from the .txt file and turn columns to rows and return the numpy array."""
    return load_data(data_file, minrows=minrows, **kwargs).T


def load_dict_from_poni(poni_file: str) -> dict:
    """Turn the poni file to pyFAI readable dictionary."""
    with Path(poni_file).open('r') as f:
        geometry = yaml.safe_load(f)
    return _lower_key(geometry)


def _lower_key(dct: Dict[str, Any]) -> Dict[str, Any]:
    """Return dictionary with all keys in lower case."""
    return {key.lower(): value for key, value in dct.items()}


def load_data(filename, minrows=10, **kwargs):
    """Find and load data from a text file.

    The data reading starts at the first matrix block of at least minrows rows
    and constant number of columns.  This seems to work for most of the
    datafiles including those generated by PDFGetX2.

    filename -- name of the file we want to load data from.
    minrows  -- minimum number of rows in the first data block.
                All rows must have the same number of floating point values.
    usecols  -- zero-based index of columns to be loaded, by default use
                all detected columns.  The reading skips data blocks that
                do not have the usecols-specified columns.
    unpack   -- return data as a sequence of columns that allows tuple
                unpacking such as  x, y = loadData(FILENAME, unpack=True).
                Note transposing the loaded array as loadData(FILENAME).T
                has the same effect.
    kwargs   -- keyword arguments that are passed to numpy.loadtxt

    Return a numpy array of the data.
    See also numpy.loadtxt for more details.
    """
    from numpy import array, loadtxt
    # determine the arguments
    delimiter = kwargs.get('delimiter')
    usecols = kwargs.get('usecols')
    # required at least one column of floating point values
    mincv = (1, 1)
    # but if usecols is specified, require sufficient number of columns
    # where the used columns contain floats
    if usecols is not None:
        hiidx = max(-min(usecols), max(usecols) + 1)
        mincv = (hiidx, len(set(usecols)))

    # Check if a line consists of floats only and return their count
    # Return zero if some strings cannot be converted.
    def countcolumnsvalues(line_):
        try:
            words = line_.split(delimiter)
            # remove trailing blank columns
            while words and not words[-1].strip():
                words.pop(-1)
            nc = len(words)
            if usecols is not None:
                nv = len([float(words[i]) for i in usecols])
            else:
                nv = len([float(w) for w in words])
        except (IndexError, ValueError):
            nc = nv = 0
        return nc, nv

    # make sure fid gets cleaned up
    with open(filename, 'rb') as fid:
        # search for the start of datablock
        start = ncvblock = None
        fpos = (0, 0)
        nrows = 0
        for line in fid:
            fpos = (fpos[1], fpos[1] + len(line))
            line = line.decode()
            ncv = countcolumnsvalues(line)
            if ncv < mincv:
                start = None
                continue
            # ncv is acceptable here, require the same number of columns
            # throughout the datablock
            if start is None or ncv != ncvblock:
                ncvblock = ncv
                nrows = 0
                start = fpos[0]
            nrows += 1
            # block was found here!
            if nrows >= minrows:
                break
        # Return an empty array when no data found.
        # loadtxt would otherwise raise an exception on loading from EOF.
        if start is None:
            rv = array([], dtype=float)
        else:
            fid.seek(start)
            # always use usecols argument so that loadtxt does not crash
            # in case of trailing delimiters.
            kwargs.setdefault('usecols', list(range(ncvblock[0])))
            rv = loadtxt(fid, **kwargs)
    return rv
